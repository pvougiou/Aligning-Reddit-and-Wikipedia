{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inspecting the Aligned Dataset</h2>\n",
    "<p>We will be inspecting the dataset of aligned Reddit sequences of comments and Wikipedia sentences. The respective HDF5 files (i.e. `reddit.h5` and `wikipedia.h5`) are built in such a way that each sequence of comments on Reddit is aligned with 20 Wikipedia sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are loading the `reddit.h5` and `wikipedia.h5` that contain the respective sequences of Reddit comments and Wikipedia sentences, aligned with each other. In those files, each word is represented by its position in the shared dictionary. We are loading that shared dictionary (i.e. `dictionary.json`) that will allow us to get the actual word given its position.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reddit_path = 'Aligned-Dataset/reddit.h5'\n",
    "wikipedia_path = 'Aligned-Dataset/wikipedia.h5'\n",
    "dictionary_path = 'Aligned-Dataset/dictionary.json'\n",
    "\n",
    "reddit = h5py.File(reddit_path, 'r')\n",
    "wikipedia = h5py.File(wikipedia_path, 'r')\n",
    "\n",
    "with open(dictionary_path, 'r') as f:\n",
    "    dictionary = json.load(f, 'utf-8')\n",
    "    id2word = dictionary['id2word']\n",
    "    id2word = {int(key): id2word[key] for key in id2word}\n",
    "    word2id = dictionary['word2id']\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def capitalise(string):\n",
    "    if string[0] == 't':\n",
    "        string = 'T'\n",
    "    else:\n",
    "        string[0] = 'V'\n",
    "    return \n",
    "\n",
    "def getAligned(index, dataset = 'train'):\n",
    "    if dataset == 'train' or dataset == 'test' or dataset == 'validate':\n",
    "        if index < len(reddit[dataset]):\n",
    "            i = 0\n",
    "            sequence = ''\n",
    "            while reddit[dataset][index][i + 1] != word2id['<PAD>']:\n",
    "                if reddit[dataset][index][i] == word2id['<end>'] or reddit[dataset][index][i] == word2id['<eot>']:\n",
    "                    sequence = sequence + id2word[reddit[dataset][index][i]].encode('utf-8', 'ignore') + '\\n'\n",
    "                else:\n",
    "                    sequence = sequence + id2word[reddit[dataset][index][i]].encode('utf-8', 'ignore') + ' '\n",
    "                i += 1\n",
    "            sequence = sequence + id2word[reddit[dataset][index][i]].encode('utf-8', 'ignore')\n",
    "            sentences = []\n",
    "            for j in range(0, 20):\n",
    "                i = 0\n",
    "                sentences.append('')\n",
    "                while wikipedia[dataset][index * 20 + j][i + 1] != word2id['<PAD>']:\n",
    "                    sentences[j] += id2word[wikipedia[dataset][index * 20 + j][i]].encode('utf-8', 'ignore') + ' '\n",
    "                    i += 1\n",
    "                sentences[j] += id2word[wikipedia[dataset][index * 20 + j][i]].encode('utf-8', 'ignore')\n",
    "\n",
    "            print ('Number: %d Sequence of Comments from the %s Set\\n' % (index, dataset.title()))\n",
    "            print (sequence)\n",
    "            print ('\\n\\nWikipedia Sentences for the Number: %d Sequence of Comments from the %s Set\\n' % (index, dataset.title()))\n",
    "\n",
    "            print ('\\n'.join(sentences))\n",
    "        else:\n",
    "            print ('The index exceeds the available examples in the %s Set.' % (dataset.title()))\n",
    "            print ('Pick an index between 0 and %d for the %s Set.' % (len(reddit[dataset]) - 1, dataset.title()))\n",
    "    else:\n",
    "        print('The available options for the dataset variable are: train, validation and test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By running the `getAligned(i, dataset)` function we are printing the $i$-th sequence of comments along with the 20 Wikipedia sentences with which it is aligned. The dataset is split into training, validation and test with respective portions of 80, 10 and 10 that result in the following options for the `dataset` variable: </p>\n",
    "* `train` containing 11248 sequences of comments along with 224960 sentences\n",
    "* `validation` containing 1406 sequences of comments along with 28100 sentences\n",
    "* `test` containing 1406 sequences of comments along with 28100 sentences\n",
    "\n",
    "<br>The `<sot>` and `<eot>` are the start-of-title and end-of-title tokens of each sequence. Each comment in a sequence is augmented with start-of-comment `<start>` and end-of-comment `<end>` tokens.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 257 Sequence of Comments from the Validate Set\n",
      "\n",
      "<sot> TIL Michael Crichton , author of Jurassic Park , felt his literature professor at Harvard was giving him unfair grades . To prove it , he turned in a paper by George Orwell and received a B- <eot>\n",
      "<start> <end>\n",
      "<start> [ deleted ] <end>\n",
      "<start> [ deleted ] <end>\n",
      "<start> Absolutely . Green Eggs and Ham is much more on point . <end>\n",
      "<start> Mind . Blown . BRB , using Green Eggs and Ham as a framework to gain insight into PalestinianIsraeli peace talks . <end>\n",
      "<start> Well , both countries can agree that it's ungodly to eat ham , with or without a side of green eggs , so progress ! A- <end>\n",
      "\n",
      "\n",
      "Wikipedia Sentences for the Number: 257 Sequence of Comments from the Validate Set\n",
      "\n",
      "Two notable examples of Communists who fought against social-chauvinism in Germany during World War I were Rosa Luxemburg and Karl Liebknecht .\n",
      "They advocated a proletarian internationalism , believing that common social relations united workers across any national boundaries .\n",
      "They stressed that the only violence the proletariat should use is the violence necessary in a socialist revolution .\n",
      "A common slogan used against social-chauvinism is \" No War but the Class War \" .\n",
      "Jeff NaN ( born Jeffrey Thomas Percy , 7 March 1970 in NaN , Manchester ) is an English actor most notable for playing Cain NaN in ITV's long-running soap opera NaN .\n",
      "Theatre 625 is a British television drama anthology series , produced by the BBC and transmitted on BBC2 from 1964 to 1968 .\n",
      "It was one of the first regular programmes in the line-up of the channel , and the title referred to its production and transmission being in the NaN NaN format , which only BBC2 used at the time .\n",
      "Kirill Sokolov ( 27 September 1930 22 May 2004 ) was a Russian painter , sculptor , NaN and stage designer .\n",
      "Name in Russian : .\n",
      "Throughout his career , Kirill NaN work and art enjoyed a wide range of media and art forms , from books illustrations and NaN printing to oil painting , collage and sculpture .\n",
      "Between 1950 and 1957 Sokolov studied at the NaN Institute in Moscow .\n",
      "Amongst his classmates was the future conceptual artist Ilya NaN .\n",
      "In 1960 Sokolov met his future wife , NaN Pyman , a British research student and biographer of Alexander Blok .\n",
      "The couple married in Moscow in 1963 .\n",
      "For the next ten years Sokolov gained distinction as a highly original engraver and illustrator of some fifty books , including the works of Mikhail Bulgakov and Yuri NaN .\n",
      "In 1974 Sokolov moved to the United Kingdom from where he established an international reputation .\n",
      "Living first in Berwick-upon-Tweed and then Durham , the artist served as co-editor of the international art journal Leonardo as well as becoming a member of the Society of Graphic Fine Art .\n",
      "During his career , Sokolov developed his own method of printmaking which he called NaN collage \" , the results of which include a tribute to George Orwell , London 1984 , and have been exhibited widely .\n",
      "Room 101 is a SBS One comedy television series hosted by Paul McDermott , based on the UK series of the same name , in which celebrities are invited to discuss their dislikes and pet hates .\n",
      "The series was scheduled to premiere on February 23 , 2015 but the network decided to delay the launch until July 11 , 2015 .\n"
     ]
    }
   ],
   "source": [
    "getAligned(257, dataset = 'validate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
